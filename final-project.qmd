---
title: "Final Project"
subtitle: "Data Science for Public Policy"
author: "Madeleine Adelson, Stephanie Jamilla, Jamie Jelly Murtha"
format: 
  html:
    code-line-numbers: true
    body-width: 1600px
embed-resources: true
editor_options: 
  chunk_output_type: console
execute: 
  warning: false
urlcolor: blue
toc: true
---

# About This Project
Climate Central's UHI index values (in Â°F) are "estimates of how much the urban built environment boosts temperatures. In other words, the UHI index is an estimate of the additional heat that local land use factors contribute to urban areas."
Stephanie 12/10 - We'll need to change the above blurb based on what temperature data we use

# Setup

Loading packages:
```{r}

library(dotenv)
library(tidyverse)
library(ggplot2)
library(sf)
library(tidycensus)
library(stringr)
library(magrittr)
library(jsonlite)
library(httr)
library(tidymodels)
library(knitr)
library(kableExtra)
library(data.table)
library(patchwork)

# Clone the private github repository for this assignment using SSH link:
# git@github.com:stejamilla/final-project.git

# do not use scientific notation
options(scipen = 999)

```


# Exploring the Data

## Cleaning DC Land Cover Data

JAMIE 12/12: I took out our removal of all vars containing "_ac" because I needed these land variables for the metrics - we can remove them later below.

Loading data from Open Data DC:
```{r}

# Loading Urban Tree Canopy by Census Block Group (2020) data
landcover <- read_sf(paste0("data/Urban_Tree_Canopy",
                            "_by_Census_Block_Group_in_2020.shp")) |>
  rename(block_group = GEOID) |>
  mutate(block_group = as.character(block_group)) |>
  mutate(OBJECTID = as.character(OBJECTID)) |>
  rename_all(tolower) |>
  relocate(block_group, .after = objectid) |>
  # Converting UHI to Fahrenheit
  mutate(uhi = (uhi*(9/5) + 32))

# Loading DC Ward data from Urban Tree Canopy by Ward (2020)
wards <- read_sf("data/Urban_Tree_Canopy_by_Ward_in_2020.shp") |>
  rename_all(tolower) |>
  select(ward) |>
  st_transform(crs = st_crs(landcover))

```

Create key with Urban Tree Canopy data column descriptions and clean up:
```{r}

landcover_key <- read_csv("data/Urban_Tree_Canopy_Descriptions.csv",
                          col_names = FALSE) |>
  mutate(X1 = str_remove_all(X1, "\\( type:.*alias:|, length:.*| \\)")) |>
  separate(col = X1,
           into = c("field", "description"),
           sep = " ",
           extra = "merge") |>
  mutate(field = tolower(field))

```

Categorizing each block group into their respective ward:
```{r}

# Spatial join so that each block group is assigned a ward based on majority area of a block group
# within a ward
landcover <- st_join(landcover, wards, largest = TRUE)

# Mapping the overlapping wards & block groups
ggplot() +
  geom_sf(data = landcover,
          aes(fill = ward))
  

```
STEPHANIE 12/13 - this code says that if a ward intersects with a blockgroup, then classify that block group as a ward. For block groups that overlap multiple wards, it returns the ward that the block group overlaps with the most area (that's the argument largest = TRUE)

## Exploring Census Demographic Data

Load ACS 5-yr 2016-2020 variables:
```{r}

acs_variables <- load_variables(2020, "acs5") |>
  filter(geography == "block group")

```

MADELEINE 12/12: I added the following variables: Population by race (B02001...), total continuous HH income (B19001_001E), median property value (B25077_001E), total property value (	
B25075_001E), median gross rent as % HH income (B25071_001E), total educational attainment (B15003_001E). Some notes/thoughts:

* We obviously don't need both the income categories and the continuous income - I just wanted to add in everything for now for data exploration. We can always drop the variables we're not using later downstream
JAMIE 12/12 - I took out the category vars, let me know if anybody wants to add back in! I think the total measure would just give us the total line from the table that breaks down all income levels (for example, see the total line here: https://data.census.gov/table/ACSDT5Y2021.B19001?q=B19001). I added the median var below.

* Also prob don't need both total and median property values - just wanted to look at both
JAMIE 12/12 - Agreed, and here too, I don't think the total line would be super helpful (see total line here: https://data.census.gov/table?q=B25075) - I removed it to avoid confusion, hope that's ok!

* Also wondering if we should use median HH income instead of total?
JAMIE 12/12 - I added in median below!

* I believe the race categories I included should sum to the total population, but will test this
JAMIE 12/12 - I think based on my reading of the Census methodologies, they normally warn that they don't sum because of estimation and error at each level. But I think this would only a problem if we're trying to sum them and use the total. 

* I'm not sure what "total educational attainment" looks like - will do some testing there too. If it's hard to interpret or not in a useful format, we can use the categorical education variables (would probably want to consolidate them into fewer categories like HS or less, some college, BA+)
JAMIE 12/12 - Here too, normally this total line is just the sum of all individuals, just to provide a total line in the table for users who only access that table and need a sum (For example it would just represent the total line in this table: https://data.census.gov/table/ACSDT1Y2023.B15003?q=B15003). As I mentioned above re: race sums, they probably won't sum exactly.

* It occurs to me we may need to choose baseline categories for any categorical variables we include and leave those out? Not sure if that is only a requirement for line and linear regression?
JAMIE 12/12 - Good point! I'm adding stuff in below without factoring that in for now, but I think we def need to determine if we need to deal with this!
STEPHANIE 12/13 - I'm not too concerned about this because we've never had to address categorical variables in the assignments. We can do something like "step_dummy()" if we want to recode a categorical variable into a dummy instead.

### Loading Census demographic data via API

JAMIE 12/12 - Unfortunately I think we have to code -66666s as NAs. See here: http://bit.ly/3DnlfIh
"-666666666: The estimate could not be computed because there were an insufficient number of sample observations...For a 5-year median estimate, the margin of error associated with a median was larger than the median itself." I'm going to take out the code turning them to 0s and put in my code converting them to NAs - let me know if you guys disagree. This starts to get into the MOE issues Professor Williams was talking about...could become a problem...

JAMIE 12/12 - We have some duplicative vars but leaving them in here. I think we can remove at step_rm to keep it clean.

```{r}

# Secure Census API key (from .env file) for use in Census API call
load_dot_env()
credential <- Sys.getenv("census_api_key")

# Create url for API call
url <- str_glue(paste0("https://api.census.gov/data/2020/acs/acs5?get=",
             # Description of data point
             "NAME,",
             # TOTAL POPULATION ESTIMATE
             "B01003_001E,",
             # TOTAL POPULATION BY RACE - WHITE ALONE
             "B02001_002E,",
             # TOTAL POPULATION BY RACE - BLACK OR AFR AMER ALONE
             "B02001_003E,",
             # TOTAL POPULATION BY RACE - AMER IND AND AK NAT ALONE
             "B02001_004E,",
             # TOTAL POPULATION BY RACE - ASIAN ALONE
             "B02001_005E,",
             # TOTAL POPULATION BY RACE - NAT HAW OR PAC ISL ALONE
             "B02001_006E,",
             # TOTAL POPULATION BY RACE - OTHER ALONE + TWO OR MORE RACES
             "B02001_008E,",
             # TOTAL HISPANIC OR LATINO
             "B03003_001E,",
             # TOTAL POP WITH DOCTORATE DEGREE
             "B15003_025E,",
             # TOTAL POP WITH PROFESSIONAL SCHOOL DEGREE
             "B15003_024E,",
             # TOTAL POP WITH MASTER'S DEGREE
             "B15003_023E,",
             # TOTAL POP WITH BACHELORS DEGREE
             "B15003_022E,",
             # TOTAL POP WITH ASSOCIATES DEGREE
             "B15003_021E,",
             # TOTAL POP WITH REGULAR HIGH SCHOOL DIPLOMA
             "B15003_017E,",
             # TOTAL POP WITH GED OR ALTERNATIVE CREDENTIAL
             "B15003_018E,",
             # MEDIAN HH INCOME IN PAST 12 MOS
             "B19013_001E,",
             # TOTAL HOUSING UNITS
             "B25002_001E,",
             # TOTAL OCCUPIED HOUSING UNITS
             "B25002_002E,",
             # TOTAL VACANT HOUSING UNITS
             "B25002_003E,",
             # MEDIAN PROPERTY VALUE
             "B25077_001E,",
             # MEDIAN GROSS RENT
             "B25064_001E,",
             # MEDIAN GROSS RENT
             # AS A PERCENTAGE OF HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B25071_001E",
             # Limit to block-group-level data for DC
             "&for=block%20group:*&for=tract:*&in=state:11&in=county:001"))

# Use url to request data from API
acs_json <- GET(url = url)

# Check call status
http_status(acs_json)

# Save API JSON response as text
acs_json <- content(acs_json, as = "text")

# Save JSON as character matrix
acs_matrix <- fromJSON(acs_json)

# Convert matrix to tibble
acs_data <- as_tibble(acs_matrix[2:nrow(acs_matrix), ],
                      .name_repair = "minimal")

# Add variable names to tibble
names(acs_data) <- acs_matrix[1, ] |>
  str_replace(" ", "_")

```

```{r}

# Clean up ACS data
acs_data <- acs_data |>
  rename(total_pop = B01003_001E,
         race_white = B02001_002E,
         race_black = B02001_003E,
         race_ai_an = B02001_004E,
         race_asian = B02001_005E,
         race_nh_pi = B02001_006E,
         race_oth_mult = B02001_008E,
         race_eth_hisp = B03003_001E,
         ed_doct = B15003_025E,
         ed_prof_deg = B15003_024E,
         ed_master = B15003_023E,
         ed_bach = B15003_022E,
         ed_assoc = B15003_021E,
         ed_ged = B15003_018E,
         ed_reg_hsd = B15003_017E,
         med_hhi = B19013_001E,
         med_prop_val = B25077_001E,
         med_gross_rent = B25064_001E,
         med_rent_pct_inc = B25071_001E,
         total_hous_units = B25002_001E,
         vac_units = B25002_003E,
         occ_units = B25002_002E,
         block_group_temp = block_group) |>
  # Create block_group column to use for combining this with the other datasets
  mutate(block_group = paste(state,
                             county, 
                             tract, 
                             block_group_temp,
                             sep = "")) |>
  rename_all(tolower) |>
  relocate(block_group, .before = name)

```

JAMIE 12/12 - Note below the issues with the acs error codes

```{r}

# Review frequency of ACS error coding (http://bit.ly/3DnlfIh)
kable(acs_codes_summary <- acs_data |>
  select(total_pop:med_rent_pct_inc) |>
  summarise(across(everything(), list(
    `sum-666666666` = ~sum(str_detect(., "-666666666"), na.rm=T),
    `sum-999999999` = ~sum(str_detect(., "-999999999"), na.rm=T),
    `sum-888888888` = ~sum(str_detect(., "-888888888"), na.rm=T),
    `sum-222222222` = ~sum(str_detect(., "-222222222"), na.rm=T),
    `sum-333333333` = ~sum(str_detect(., "-333333333"), na.rm=T),
    `sum-555555555` = ~sum(str_detect(., "-555555555"), na.rm=T),
    `sum*` = ~sum(. == "*", na.rm=T),
    sumnull = ~sum(. == "null", na.rm=T),
    sumNA = ~sum(is.na(.) == TRUE),
    sumNA_Char = ~sum(. == "NA", na.rm=T)))) |>
  pivot_longer(cols = everything(), 
               names_to = c("colNames", ".value"),
               names_sep="_sum") |>
  filter(rowSums(across(where(is.numeric)))>0))

# Redefine coded values
# Create list of codes in character format
acs_codes_char <- c("-666666666",
                   "-999999999",
                   "-888888888",
                   "-222222222",
                   "-333333333",
                   "-555555555",
                   "*",
                   "null")

# Replace the codes with NA
acs_data <- acs_data |> 
  mutate(across(where(is.character), 
                ~if_else(str_detect(.,
                                    "-666666666|-999999999|-888888888|-222222222|-333333333|-555555555|null"),
                         NA_character_, .))) |>
  # convert all to numeric |>
  mutate_at(vars(total_pop:med_rent_pct_inc), as.numeric)

```

# Creating the Final Dataset for Analysis

STEPHANIE 12/13 - I had to move this up and change the dataset from dc_train to landcover because we have to show how we determine the threshold before using mutate on it in the following code chunk.

Deciding the threshold for hot vs. not hot:
```{r}

# visualizing a cumulative histogram of counts of UHI values
landcover |>
  group_by(uhi) |>
  ggplot(aes(uhi)) +
  geom_histogram(aes(y = cumsum(..count..)))

# calculating frequency & cumulative frequency
heat_freq <- landcover |>
  count(uhi)

heat_freq$cumulative <- cumsum(heat_freq$n)

heat_freq |>
  filter(
    cumulative >= 285,
    cumulative <= 286) |>
  head()

```

JAMIE 12/12: Note I changed the name of the main df we are using pre-split to "dc_full_data." ALSO note that I moved the hotspot / not hotspot var to the below code chunk to streamline data cleanup.

STEPHANIE 12/13 - There's something odd happening with race_eth_hisp_ratio, because I'm looking at the training data and the values are all 1 (or NaN), which presumably means that the population share of hispanic/latino is 100% for each block group, which presumably wouldn't happen??

Moved your threshold notes here:

MADELEINE 12/12
New 50% threshold (UHI data from landcover; C to F conversion) is 90.1.

STEPHANIE 12/13
Did we want to go with 25% for hotspot vs 75%? I'm fine to change the threshold, we'll just have to justify which level to go for.

Also, here's the data that has missing values (as per a warning message from random forests) - Missing data in columns: race_white_ratio, race_ai_an_ratio, race_asian_ratio, race_oth_mult_ratio, ed_hsd_ratio, ed_bach_ratio, ed_adv_ratio, med_hhi, med_prop_val, med_rent_pct_inc, hous_units_per_person, vac_units_share, inc_home_val_ratio.

```{r}

# Create final dataset and add metrics
dc_full_data <- left_join(landcover, acs_data, by = "block_group") |>
  mutate(
    # convert total acres to sq miles
    total_sq_mi = total_ac * 0.0015625,
    # convert water acres to sq miles
    water_sq_mi = wat_ac * 0.0015625,
    # calculate water share of total sq miles
    share_water_sq_mi = water_sq_mi / total_sq_mi,
    # calculate population density per square mile
    pop_dens_sq_mi = total_pop / total_sq_mi,
    # calculate population density per water sq mile
    pop_density_water_sq_mi = total_pop / water_sq_mi,
    # calculate ratio of housing units to population
    hous_units_per_person = total_hous_units / total_pop,
    # calculate ratio of vacant units to all units
    vac_units_share = vac_units / total_hous_units,
    # calculate ratio of total income to median home value
    inc_home_val_ratio = med_hhi / med_prop_val,
    # create combined HS diploma or equivalent variable
    ed_comb_hsd = ed_reg_hsd + ed_ged,
    # create combined advanced post-undergraduate variable
    ed_comb_adv_deg = ed_doct + ed_prof_deg + ed_master,
    # calculate ratio of 25+ pop with HSD to total pop
    ed_hsd_ratio = ed_comb_hsd / total_pop,
    # calculate ratio of 25+ pop with bachelor's degree to total pop
    ed_bach_ratio = ed_bach / total_pop,
    # calculate ratio of 25+ pop with advanced post-undergrad degree to total pop
    ed_adv_ratio = ed_comb_adv_deg / total_pop,
    # calculate pop share white
    race_white_ratio = race_white / total_pop,
    # calculate pop share black or african american
    race_black_ratio = race_black / total_pop,
    # calculate pop share american indian or alaska native
    race_ai_an_ratio = race_ai_an / total_pop,
    # calculate pop share asian
    race_asian_ratio = race_asian / total_pop,
    # calculate pop share native hawaiian or pacific islander
    race_nh_pi_ratio = race_nh_pi / total_pop,
    # calculate pop share two or more races
    race_oth_mult_ratio = race_oth_mult / total_pop,
    # calculate pop share hispanic or latino
    race_eth_hisp_ratio = race_eth_hisp / total_pop,
    # calculate total non-white population
    race_poc = race_black + race_ai_an + race_asian + race_nh_pi + race_oth_mult,
    # calculate pop share non-white
    race_poc_ratio = race_poc / total_pop,
    # creating hotspot var for decision tree model
    hotspot = if_else(uhi >= 90.1, "hotspot", "not hotspot"),
    # remove factor from uhi var
    uhi = as.numeric(as.character(uhi))) |>
  relocate(c("block_group_temp", "tract"), .after = 2) |>
  # remove columns with only one unique value
  select(-objectid,
         -name,
         -state,
         -statefp,
         -county,
         -countyfp,
         -tractce,
         -blkgrpce,
         -namelsad,
         -mtfcc,
         -funcstat,
         -field,
         -shapearea,
         -shapelen) |>
  relocate(race_white_ratio:race_eth_hisp_ratio, .after = race_eth_hisp) |>
  relocate(ed_comb_hsd:ed_adv_ratio, .after = ed_ged) |>
  relocate(hous_units_per_person:inc_home_val_ratio, .after = med_rent_pct_inc) |>
  relocate(total_sq_mi:pop_density_water_sq_mi, .after = inc_home_val_ratio) |>
  relocate(total_pop:hotspot, .after = tract)

```

## Prepare the data for analysis (set seed & splitting)

JAMIE 12/12 - Changed the var names so that these are the splits we use for all models, not just decision tree

* Do we need an implementation set too?

STEPHANIE 12/13 - Aaron mentioned that we should make sure that when we split the data, that we should stratify the data by some geographic feature so that for example not all the block groups in a certain ward are only in the training data. Because then our model would only really work for that ward. Thus, the argument strata = ward.
* Concerning implementation data, I worry that since we only have a set of n = 571 that taking out a number would significantly affect our model. Maybe we can just do a set of 10 data points for the implementation data like we did in Assignment 7, but we'd have to randomize which ten are taken out (which we didn't do in assignment 7, we sliced the data by where observations were located in the dataset)


```{r}

# Set seed
set.seed(20241202)

# Split sample and create training and testing datasets
dc_split <- initial_split(data = dc_full_data, strata = ward, prop = 0.8)
dc_train <- training(x = dc_split) 
dc_test <- testing(x = dc_split)

```

# Initial EDA

## Review summary statistics

JAMIE 12/12 - Added summary stats table here

```{r}

# Review summary statistics
kable(dc_train_summary <- as_tibble(dc_train |>
                                      select(where(is.numeric)) |>
                                      lapply(summary) |>
                                      lapply(`length<-`, 6)) |>
        mutate(measure = c("Min", "Q1", "Median", "Mean", "Q3", "Max")) |>
        mutate(across(everything(), as.vector)) |>
        pivot_longer(-measure) |>
        pivot_wider(id_cols = "name",
                    names_from = "measure",
                    values_from = "value"))

```

## Exploration of some of the continuous variables

JAMIE 12/12 - when we run the below viz we have a warning message that 56 rows were removed.

* I silenced the total ed attainment ggplot per by above note.

STEPHANIE 12/13 - concerning the rows being removed, I think that's fine. I think that's happened with me for other visualizations before.
* Concerning the ed attainment, I'm getting a thing that "educ_total" doesn't exist...so I'm not sure what's happening here


```{r}

# Median household income, continuous
dc_train |>
  ggplot() +
  geom_histogram(aes(x = as.numeric(med_hhi))
  )

# Total educational attainment
#dc_train |>
#  ggplot() +
#  geom_histogram(aes(x = as.numeric(educ_total))
#  )
# Still not sure how to interpret this one. Might be worth going back to the categories

```

## Exploring temperature in relation to other variables

JAME 12/12 - I fixed this - I guess we have to left_join with the sf df in the first position in left_join(), so that we join the non-sf df onto the sf df and it stays sf. BUT do we have to st_transform() since the crs is not 4326?

STEPHANIE 12/13 - It doesn't matter that the CRS isn't 4326 for mapping purposes - the main concern with CRS is that it has to be the same between two spatial datasets if you're going to do something like join the two. 
I'm also realizing that we probably shouldn't do a map here with dc_train because this map shows which block groups are in the training data (vs. the testing data) since you can see which block groups are excluded from the map...so this may be a form of data leakage. I'm going to instead use the dc_full_data dataset.

```{r}

dc_full_data |> ggplot() +
  geom_sf(
    mapping = aes(fill = uhi),
    color = "#400001"
    ) +
    scale_fill_gradient(
    low = "#f7ef9d",
    high = "#be2b25"
    ) +
  labs(
    title = "Average evening temperature, 2018",
    fill = "Temperature (degrees F)"
  ) +
  theme_void()

```

Temperature & population:
```{r}

# Total population
dc_train |>
  ggplot(aes(x = total_pop, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()

# Population density
dc_train |>
  ggplot(aes(x = pop_dens_sq_mi, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()

# Non-white population density
dc_train |>
  ggplot(aes(x = race_poc_ratio, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()

```

Temperature & income/education
```{r}

# Median household income
dc_train |>
  ggplot(aes(x = med_hhi, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()

# Ratio of bachelors degrees
dc_train |>
  ggplot(aes(x = ed_bach_ratio, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()


```

Temperature & landcover
```{r}

# Comparing with percentage of non-canopy vegetation
dc_train |>
  ggplot(aes(x = veg_pct, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()

# Comparing with percentage area of urban tree canopy
dc_train |>
  ggplot(aes(x = utc_pct, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()

# Comparing with percentage area of impervious surfaces (e.g., roads, parking lots, etc.)
dc_train |>
  ggplot(aes(x = to_ia_pct, y = uhi, color = hotspot)) +
  geom_point(alpha = 0.5) +
  theme_minimal()


```

STEPHANIE 12/13 - The graphs that compare UHI with percentage area of urban tree canopy and of impervious surfaces seems quite compelling. This makes sense, and it's intersting to notice how not a lot of the demographic stuff doesn't seem to show up in EDA but the landcover stuff seem like promising predictors.

# Decision Tree

## Modelling
STEPHANIE 12/13 - I added "-tract" and "-ward" to remove more geographic indicators that likely would affect the model, since that otherwise would've been a main predictor of a hotspot

```{r}

# Prep the dataset for modelling
dc_train <- dc_train |>
  st_drop_geometry() |>
  select(-block_group, -intptlat, -intptlon, -gis_id, -globalid, gid, -tract, -ward, -block_group_temp) 

```


JAMIE 12/12 - Changing the dataset used in the recipe to dt_train rather than the full df
* In the recipe for the lm and rf models, I included a lot of variables in step_rm. Wanted to get your thoughts on these before I added them here too...Just removed uhi and pop_density_water_sq_mi for now.

STEPHANIE 12/13 - I think what you removed here is fine. I've decided to keep in downsampling because I think it's useful. For reference, down-sampling "randomly subset all the classes in the training set so that their class frequencies match the least prevalent class. For example, suppose that 80% of the training set samples are the first class and the remaining 20% are in the second class. Down-sampling would randomly sample the first class to be the same size as the second class (so that only 40% of the total training set is used to fit the model)."

```{r}

dt_recipe <- recipe(formula = hotspot ~ ., data = dc_train) |>
  step_rm(
    # remove uhi var to avoid duplication of data from hotspot
    uhi,
    # remove pop_density_water_sq_mi due to inf value for most records 
    pop_density_water_sq_mi) |>
  themis::step_downsample(hotspot) |> 
  step_nzv(all_numeric_predictors())

dt_model <- decision_tree() |>
  set_engine(engine = "rpart") |>
  set_mode(mode = "classification")

dt_workflow <- workflow() |>
  add_recipe(dt_recipe) |>
  add_model(dt_model)
  
dt_fitting <- dt_workflow |>
  fit(data = dc_train)

rpart.plot::rpart.plot(x = dt_fitting$fit$fit$fit,
                       roundint = FALSE)

```
STEPHANIE 12/13 - 
I think we have an interesting model. The first decision point is whether the percentage of the block group that's unsuitable for planting; if it's lower than 44% then it's not a hotspot. If it is greater than or equal to 44%, then you check if it's greater than or equal to 55%; if it is then it's a hotspot. If it's not then you check if the total acre area of buildings, and if it's less than 6.4 acres, then it's not a hotspot. If it is, then you check the percentage of area that's a parking lot. If it's greater than or equal to 1.6%, then it's a hotspot. 

Notably there's no demographic indicators here, it's all land cover based. Part of me wonders if we should take out the total area (_ac) variables, but I think it's kinda interesting keeping it in. I wonder why total area became a decision branch rather than the percentage version.

## Visualizing Important Variables

STEPHANIE 12/13 - Using dc_full_data as the dataset since it's possible we may accidentally do data leakage when mapping dc_train because we'll notice which block groups were excluded.

```{r fig.width = 10, fig.height = 8}

planting_map <- dc_full_data |> ggplot() +
  geom_sf(aes(fill = to_un_pct)) +
    scale_fill_gradient(
    low = "#fcbc32",
    high = "#4e974b"
    ) +
  theme_minimal() +
  labs(
    title = "Percentage of area suitable for planting (2020)",
    subtitle = "by Block Group",
    fill = "Percent Area",
    caption = "Open Data DC"
  )

building_map <- dc_full_data |> ggplot() +
  geom_sf(aes(fill = bld_ac)) +
    scale_fill_gradient(
    low = "#c4c8c0",
    high = "#643424"
    ) +
  theme_minimal() +
  labs(
    title = "Total area of buildings in acres (2020)",
    subtitle = "by Block Group",
    fill = "Total Area",
    caption = "Open Data DC"
  )

lot_map <- dc_full_data |> ggplot() +
  geom_sf(aes(fill = plot_pct)) +
    scale_fill_gradient(
    low = "#28292b",
    high = "#f8a700"
    ) +
  theme_minimal() +
  labs(
    title = "Percentage of area of parking lots (2020)",
    subtitle = "by Block Group",
    fill = "Percent Area",
    caption = "Open Data DC"
  )

planting_map + building_map + lot_map

```

# Linear Regression

JAMIE 12/12 - Do we want folds? added here. Not using folds does not make the lm rank deficient model warning go away.
STEPHANIE 12/13 - I think yes, we'll have to use it, especially for hyper parameter tuning. My main issue with folding at this point is that I think we need to remove all the variables we don't want from dc_train first and then get do folding so that the folding process doesn't take them into account. So I think we should take out the variables from dc_train before the lm and rf models (using select rather than step_rm) and then do the folding.

```{r}

# Editing dc_train data for the sake of folding & hyperparameter tuning
dc_train <- dc_train |>
  select(
    # remove raw total pop var (custom pop density and other pop proportion
    # vars remain)
    -total_pop,
    # remove raw race vars (custom proportion vars remain)
    -race_white,
    -race_black,
    -race_ai_an,
    -race_asian,
    -race_nh_pi,
    -race_oth_mult,
    -race_eth_hisp,
    # remove raw ed vars (custom proportion vars remain)
    -ed_doct,
    -ed_prof_deg,
    -ed_master,
    -ed_bach,
    -ed_assoc,
    -ed_reg_hsd,
    -ed_ged,
    -ed_comb_hsd,
    -ed_comb_adv_deg,
    # remove duplicative housing data (custom proportion vars and pct var
    # remain)
    -total_hous_units,
    -occ_units,
    -vac_units,
    -med_gross_rent,
    # remove duplicative land data (custom proportion / "pct" vars remain)
    -ends_with("_ac"),
    -total_sq_mi,
    -water_sq_mi,
    -wtr_pct,
    -aland,
    -awater,
    -utcac0620,
    -utcac1120,
    -utcac1520,
    -utcpct0620,
    -utcpct1120,
    -utc_pct_06,
    -utc_pct_11,
    -utc_pct_15,
    # remove duplicative hotspot
    -hotspot,
    # remove pop_density_water_sq_mi due to INF value for most records 
    -pop_density_water_sq_mi)

# V-fold cross validation with 10 folds
dc_folds <- vfold_cv(data = dc_train, v = 10)

```


JAMIE 12/12 - Added recipe for lm and rf and added lm model and fit below. Having trouble with this major warning:
A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient="NA")
Also, I experimented here with removing all the columns that seemed duplicative. I did not add this to the dt recipe for now. Let me know what you all think of removing these.

STEPHANIE 12/13 - The main issue are the columns that have NA values. We need to decide if we should remove those or if it's possible for us to do data imputation aka decide what those values should be.
I removed the identifying factor variables becuase I had to remove them earlier from dc_train.

```{r}

# Create recipe for use in linear regression and random forest models
# Create the recipe
dc_recipe <- recipe(formula = uhi ~ ., data = dc_train) |>
  # Removing predictors that have near zero variability
  step_nzv(all_predictors()) |>
  # Removing predictors that are highly correlated with others
  step_corr(all_predictors())

  

```

```{r}

# Model 2 - linear regression
dc_lm_model <- 
  linear_reg() |>
  set_mode(mode = "regression") |>
  set_engine(engine = "lm")

# Linear regression workflow
dc_lm_workflow <- 
  workflow() |>
  add_recipe(recipe = dc_recipe) |>
  add_model(spec = dc_lm_model)
  
unloadNamespace("Metrics")

# Fit the regression workflow
dc_lm_fit <-
  dc_lm_workflow |>
  fit_resamples(
    resamples = dc_folds,
    metrics = metric_set(rmse),
    control = control_resamples(save_workflow = TRUE))

```

# Random Forests

## Modelling 

### Hyperparameter Tuning

STEPHANIE 12/13

```{r}

dc_rf_model <- 
  rand_forest(
  trees = 200,
  mtry = tune(),
  min_n = tune()) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

# Create a parameter grid
kable(dc_rf_grid <- grid_regular(mtry(range = c(1, 15)),
                                min_n(range = c(1, 15)),
                                levels = 5))

# Random forest workflow
dc_rf_workflow <- 
  workflow() |>
  add_recipe(dc_recipe) |>
  add_model(dc_rf_model)

# Creating a grid to show results of hyper parameter tuning
dc_rf_grid <- grid_regular(
  mtry(range = c(1, 15)),
  min_n(range = c(1, 15)),
  levels = 5
)

# Tuning the grid
rf_resamples <- tune_grid(
  dc_rf_workflow,
  resamples = dc_folds,
  grid = dc_rf_grid
)

# Showing the best hyperparameters
show_best(rf_resamples)

```

### Implementing best hyperparameters based on tuning

```{r}

# Filling in hyperparameters based on tuning
dc_rf_model <- rand_forest(
  trees = 200,
  mtry = #fill in based on the best resample,
  min_n = #fill in based on the best resample
) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

# Random forest workflow
dc_rf_workflow <- 
  workflow() |>
  add_recipe(dc_recipe) |>
  add_model(dc_rf_model)

rf_resamples <- dc_rf_workflow |>
  fit_resamples(resamples = dc_folds)

rf_resamples |>
  collect_metrics(summarize = FALSE) |>
  filter(.metric == "rmse") |>
  print() |>
  ggplot(mapping = aes(x = id, y = .estimate, group = .metric)) +
  geom_point() +
  geom_line()

```

## Data Visualization
```{r}



```

# Comparing Models
## Plot the RMSEs across each fit:

JAMIE 12/12 - Below is what I used in the assignment 07 hw to pull the metrics in the end, so pasted here and updated a bit. It renders!

```{r}

# Create a df with rmses for all models

# Extract rmses for rf approach
rf <- as_tibble(
  sapply(dc_rf_fit$.metrics, function(x) extract2(x, 1))) |>
  select(V1) |>
  rename(mtry = V1) |>
  bind_cols(as_tibble(
  sapply(dc_rf_fit$.metrics, function(x) extract2(x, 2)))) |>
  select(mtry, V1) |>
  rename(min_n = V1) |>
  bind_cols(as_tibble(
  sapply(dc_rf_fit$.metrics, function(x) extract2(x, 5)))) |>
  rename_all( ~ str_replace(., "V", "Fold0")) |>
  rename(Fold10 = Fold010) |>
  pivot_longer(cols = 3:12,
               names_to = "id",
               values_to = "rmse") |>
  mutate(model = "rf") |>
  select(id, mtry, min_n, model, rmse)

# Extract rmses for lm approaches
lm <- tibble(
  id = dc_lm_fit$id,
  lm = as_vector(sapply(dc_lm_fit$.metrics, function(x) extract2(x, 3)))) |>
  pivot_longer(cols = c("lm"),
               names_to = "model",
               values_to = "rmse") |>
  mutate(nn = NA,
         mtry = NA,
         min_n = NA) |>
  select(id, mtry, min_n, model, rmse)

# Create one df containing all rmses
fit_comps <- bind_rows(lm, rf)

# Plot the rmses
fit_comps |>
  ggplot(mapping = aes(x = model, y = rmse)) +
  geom_point() +
  labs(title = "RMSE of Linear Regression and Random Forest Models\n",
       subtitle = paste0("Prediction Model Evaluation for Washington, DC\n",
                         "Urban Heat Index (UHI)"),
       x = "Prediction Model",
       y = "RMSE")

# print average rmse per model
kable(avg_rmse <- tibble(
  model = c("lm", "rf"),
  rmse = c(mean(fit_comps$rmse[fit_comps$model=="lm"]),
           mean(fit_comps$rmse[fit_comps$model=="rf"]))) |>
    arrange(rmse))

```

## Estimate the out-of-sample error rate

JAMIE 12/13 - Added metrics work below

```{r}

library(Metrics)

# save best lm fit
best_lm_fit <- fit_best(dc_lm_fit, verbose = TRUE)

# make predictions using best fit
test_predictions <-
  dc_test |>
  select(uhi) |>
  rename(y = uhi) |>
  mutate(y = as.numeric(y)) |>
  mutate(y_hat = as.numeric(as_vector(predict(best_lm_fit, dc_test))))

# collect rf metrics
kable(rmse(actual = test_predictions$y, predicted = test_predictions$y_hat))

```


# Sources

https://www.climatecentral.org/climate-matters/urban-heat-islands-2024

https://opendata.dc.gov/datasets/DCGIS::urban-tree-canopy-by-census-block-group-in-2020/about

Census ACS 5yr 2016 - 2020

https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/MultiyearACSAccuracyofData2020.pdf


