---
title: "Final Project"
subtitle: "Data Science for Public Policy"
author: "Madeleine Adelson, Stephanie Jamilla, Jamie Jelly Murtha"
format: 
  html:
    code-line-numbers: true
    body-width: 1600px
embed-resources: true
editor_options: 
  chunk_output_type: console
execute: 
  warning: false
urlcolor: blue
toc: true
---

## About This Project
Climate Central's UHI index values (in Â°F) are "estimates of how much the urban built environment boosts temperatures. In other words, the UHI index is an estimate of the additional heat that local land use factors contribute to urban areas."

Stephanie 12/10 - We'll need to change the above blurb based on what temperature data we use


## Setup

Loading packages:
```{r}

library(dotenv)
library(tidyverse)
library(ggplot2)
library(sf)
library(tidycensus)
library(stringr)
library(jsonlite)
library(httr)
library(tidymodels)
library(sp) # Make sure to use install.packages("sp") before using this line of code
library(raster) # Make sure to use install.packages("raster") before using this line of code

# Clone the private github repository for this assignment using SSH link:
# git@github.com:stejamilla/final-project.git

# do not use scientific notation
options(scipen = 999)

```


# Exploring the Data

## Cleaning DC Land Cover Data
```{r}

# Read in Urban Tree Canopy by Census Block Group (2020) data from Open Data DC
# and clean up
landcover <- read_sf(paste0("data/Urban_Tree_Canopy",
                            "_by_Census_Block_Group_in_2020.shp")) |>
  rename(block_group = GEOID) |>
  # Drop other unrelated UHI index, which, here, measures average temperature
  # within each feature
  mutate(block_group = as.character(block_group)) |>
  rename_all(tolower) |>
  # remove columns that provide measures in acres, to focus instead on
  # percentage measures
  dplyr::select(-contains("_ac")) |>
  relocate(block_group, .after = objectid)

# Create key with Urban Tree Canopy data column descriptions and clean up
landcover_key <- read_csv("data/Urban_Tree_Canopy_Descriptions.csv",
                          col_names = FALSE) |>
  mutate(X1 = str_remove_all(X1, "\\( type:.*alias:|, length:.*| \\)")) |>
  separate(col = X1,
           into = c("field", "description"),
           sep = " ",
           extra = "merge") |>
  mutate(field = tolower(field))
  
```
STEPHANIE 12/10 - I've readded UHI since we may use it if the temperature data doesn't work out. Also, the "raster" package/library also has its own select() function, so if we mean the dplyr select() function moving forward, we'll have to use dplyr::select(). Also moved this up before the temperature data since we need to load the geometry data to then use with the temp data below.

## Cleaning Temperature Data

Initial testing of raster data manipulation (to delete)
```{r}

# Loading average July 2020 land surface temperature data
temp_test <- raster("data/MOD_LSTD_M_2020-07-01_rgb_3600x1800.FLOAT.TIFF")

# Pulling out the geometry from the landcover data
dc <- landcover |>
  dplyr::select(geometry, block_group) |>
  st_transform(crs = crs(temp_test)) # Aligning coordinate systems

# Aligning coordinate systems
heat <- extract(temp_test, dc, fun = mean)

# Creating temperature dataset
dc_temp <- bind_cols(heat, dc) |>
  rename(temp = ...1) 

dc_temp_sf <- st_as_sf(dc_temp) 

dc_temp |>
  count(temp)

# Visualizing temperature data
dc_temp_sf |>
  ggplot() +
  geom_sf(mapping = aes(fill = temp)) +
  scale_fill_gradient(
    'Temperature',
    low = 'blue',
    high = 'red'
  ) +
  labs(title = "DC Average July 2020 Temperature") +
  theme_minimal() 

# Exploring UHI variable
st_drop_geometry(landcover) |>
  count(uhi)

```
Stephanie 12/10 - Downloaded temperature data from NASA's Earth Observations: https://neo.gsfc.nasa.gov/view.php?datasetId=MOD_LSTD_M&date=2020-07-01

## Exploring Census Demographic Data

```{r}

# Load ACS 5-yr 2016-2020 data
variables <- load_variables(2020, "acs5") |>
  filter(geography == "block group")

```
STEPHANIE PRE-12/3
I (Stephanie) think we could use some or all of the following ACS data:
* B01003_001 - Total population
* B02001_002 through B02001_010 - Population for different racial groups
* B15003_002 through B15003_025 - Educational attainment of those 25 and over
* B19001_002 through B19001_017 - Household income in the past 12 months
* B25001_001 - Total # of housing units
* B25002_002 & B25002_003 - Total # of occupied or vacant units
* B25075_002 through B25075_027 - Total value of housing units

I'll only load a few below for analysis since we haven't agreed on the variables yet, but I want to be able to play around with some data. -Stephanie

JAMIE 12/3 - I like these variables! Here are some ideas to scale them and make them relatable across areas and years:
* B01003_001 - Total population
- Log population growth since the previous 5-yr ACS
- Population per square mile or other distance (density)
- Population per total # of housing units

* B02001_002 through B02001_010 - Population for different racial groups
- Same thoughts as above

* B15003_002 through B15003_025 - Educational attainment of those 25 and over
- Share of population with each level of educational attainment
- Population with each level of educational attainment per square mile, or other distance (density of educational attainment)
- Population with each level of educational attainment per total # of housing units

* B19001_002 through B19001_017 - Household income in the past 12 months
- We just need to put in terms of one year's dollars - such as 2020 dollars
- Maybe we can use a metric that standardizes this relative to the living wage in the area (using a source like the MIT living wage calculator)

* B25075_002 through B25075_027 - Total value of housing units
- Same thoughts as above, need to put in terms of one year's dollars

If these are of interest, I'm happy to pull the data and add the calculations.

MADELEINE 12/4:
I think all the variables listed above sound good. I'd perhaps add either
* B25064_001 - Median gross rent, or
* B25071_001 - Median gross rent as a percentage of household income

Thinking that these help provide more of a sense of what it actually costs to live in that neighborhood vs. just the value of property (though I also think value is important).

A couple questions:
- I'm not sure we need to convert to 2020 dollars since it seems like that may have already been done? If you look at the concept column for the B19001 vars, for instance, it says "in 2020 inflation-adjusted dollars".
- Is there a reason we are planning to use the bracketed variables instead of continuous? for instance, B19001_01 vs. 02 through 017

Stephanie 12/4
Remember we have to think about weights!

```{r}

# Secure Census API key (from .env file) for use in Census API call
load_dot_env()
credential <- Sys.getenv("census_api_key")

### I think since we're not using the tidycensus package to obtain the ACS data,
### we don't need this code
# census_api_key(credential, install = TRUE, overwrite = TRUE)

# Create url for API call
url <- str_glue(paste0("https://api.census.gov/data/2020/acs/acs5?get=",
             # Description of data point
             "NAME,",
             # TOTAL POPULATION ESTIMATE
             "B01003_001E,",
             # TOTAL LESS THAN $10,000
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_002E,",
             # TOTAL $10,000 - $14,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_003E,",
             # TOTAL $15,000 - $19,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS   
             "B19001_004E,",
             # TOTAL $20,000 - $24,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS             
             "B19001_005E,",
             # TOTAL $25,000 - $29,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_006E,",
             # TOTAL $30,000 - $34,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS      
             "B19001_007E,",
             # TOTAL $35,000 - $39,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_008E,",
             # TOTAL $40,000 - $44,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_009E,",
             # TOTAL $45,000 - $49,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_010E,",
             # TOTAL $50,000 - $59,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_011E,",
             # TOTAL $60,000 - $74,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS       
             "B19001_012E,",
             # TOTAL $75,000 - $99,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_013E,",
             # TOTAL $100,000 - $124,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_014E,",
             # TOTAL $125,000 - $149,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_015E,",
             # TOTAL $150,000 - $199,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_016E,",
             # TOTAL $200,000 or more
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_017E",
             # Limit to block-group-level data for DC
             "&for=block%20group:*&for=tract:*&in=state:11&in=county:001"))

# Use url to request data from API
acs_json <- GET(url = url)

# Check call status
http_status(acs_json)

# Save API JSON response as text
acs_json <- content(acs_json, as = "text")

# Save JSON as character matrix
acs_matrix <- fromJSON(acs_json)

# Convert matrix to tibble
acs_data <- as_tibble(acs_matrix[2:nrow(acs_matrix), ],
                      .name_repair = "minimal")

# Add variable names to tibble
names(acs_data) <- acs_matrix[1, ] |>
  str_replace(" ", "_")

# Clean up data
acs_data <- acs_data |>
  rename(total_pop = B01003_001E,
         nc_less_10000 = B19001_002E,
         inc_10000_14999 = B19001_003E,
         inc_15000_19999 = B19001_004E,
         inc_20000_24999 = B19001_005E,
         inc_25000_29999 = B19001_006E,
         inc_30000_34999 = B19001_007E,
         inc_35000_39999 = B19001_008E,
         inc_40000_44999 = B19001_009E,
         inc_45000_49999 = B19001_010E,
         inc_50000_59999 = B19001_011E,
         inc_60000_74999 = B19001_012E,
         inc_75000_99999 = B19001_013E,
         inc_100000_124999 = B19001_014E,
         inc_125000_149999 = B19001_015E,
         inc_150000_199999 = B19001_016E,
         inc_200000_more = B19001_017E,
         block_group_temp = block_group) |>
  # Create block_group column to use for combining this with the other datasets
  mutate(block_group = paste(state,
                             county, 
                             tract, 
                             block_group_temp,
                             sep = "")) |>
  rename_all(tolower) |>
  relocate(block_group, .before = name)
                             
```

## Stitching Together Final Dataset

Stephanie 12/11 - We'll have to change the bottom code due to using different temperature data

```{r}

dataset_interim <- left_join(acs_data, st_drop_geometry(dc_heat), by = "block_group")
dataset_final <- left_join(dataset_interim, landcover, by = "block_group")

# Checking to make sure we have all the variables
colnames(dataset_final)

```

## Initial EDA
### DC Temperature Map

Stephanie 12/11 - We'll have to change the bottom code due to using different temperature data

```{r}


```

# Supervised Machine Learning Applications

## Decision Trees

Stephanie 12/11 - We'll have to change the bottom code due to using different temperature data

Deciding the threshold for hot vs. not hot:
```{r}

# visualizing a cumulative histogram of counts of UHI values
dc_heat |>
  group_by(uhi) |>
  ggplot(aes(uhi)) +
  geom_histogram(aes(y = cumsum(..count..)))

# calculating frequency & cumulative frequency
heat_freq <- dc_heat |>
  count(uhi)
heat_freq$cumulative <- cumsum(heat_freq$n) 

```
STEPHANIE PRE-12/3
Since there's 571 block groups, that means if we want to decide the heat threshold is where 50% of the data is above/below this line, then we'd want to pick the UHI value where cumulative = 285 or 286. The closest datapoint is UHI = 7.9 (Where cumulative = 268). I vote that we pick UHI <= 7.9 means not a heat island whereas UHI > 7.9 means it is a heat island.

MADELEINE 12/4
We might want to consider a higher threshold? I'm thinking it would be more meaningful to point out, say, the hottest 25% of block groups vs. the top 50%.

Implementing threshold:
```{r}

dataset_final <- dataset_final |>
  mutate(hotspot = if_else(uhi <= 7.9, "Hotspot", "Not Hotspot"))

dataset_dt <- dataset_final |>
  st_drop_geometry() |>
  select(-uhi, -geometry, -block_group, -name, -intptlat, -intptlon) 
#dropping uhi because that would be the main decision tree factor

```

### Running decision tree
STEPHANIE PRE-12/3
Note: We should do some more data cleaning on dataset_final before running the decision tree but it's getting late and I am tired, so I'm leaving it as is for now -Stephanie
```{r}

set.seed(20241202)

dt_split <- initial_split(data = dataset_dt, prop = 0.8)
dt_train <- training(x = dt_split) 
dt_test <- testing(x = dt_split)

dt_recipe <- recipe(formula = hotspot ~ ., data = dataset_dt) |>
  themis::step_downsample(hotspot) |> #I don't know if we need this or not...
  step_nzv(all_numeric_predictors())

dt_model <- decision_tree() |>
  set_engine(engine = "rpart") |>
  set_mode(mode = "classification")

dt_workflow <- workflow() |>
  add_recipe(dt_recipe) |>
  add_model(dt_model)
  
dt_fitting <- dt_workflow |>
  fit(data = dt_train)

rpart.plot::rpart.plot(x = dt_fitting$fit$fit$fit,
                       roundint = FALSE)

```
Stephanie 12/4
Looks like block group is the main determinant, which makes sense haha - should probably remove that column.

## Linear Regression
```{r}



```

## Random Forests
```{r}



```


## Sources

https://www.climatecentral.org/climate-matters/urban-heat-islands-2024

https://opendata.dc.gov/datasets/DCGIS::urban-tree-canopy-by-census-block-group-in-2020/about

Census ACS

