---
title: "Final Project"
subtitle: "Data Science for Public Policy"
author: "Madeleine Adelson, Stephanie Jamilla, Jamie Jelly Murtha"
format: 
  html:
    code-line-numbers: true
    body-width: 1600px
embed-resources: true
editor_options: 
  chunk_output_type: console
execute: 
  warning: false
urlcolor: blue
toc: true
---

## About This Project
Climate Central's UHI index values (in Â°F) are "estimates of how much the urban built environment boosts temperatures. In other words, the UHI index is an estimate of the additional heat that local land use factors contribute to urban areas."
Stephanie 12/10 - We'll need to change the above blurb based on what temperature data we use

## Setup

Loading packages:
```{r}

library(dotenv)
library(tidyverse)
library(ggplot2)
library(sf)
library(tidycensus)
library(stringr)
library(jsonlite)
library(httr)
library(tidymodels)
library(knitr)
library(data.table)

# Clone the private github repository for this assignment using SSH link:
# git@github.com:stejamilla/final-project.git

# do not use scientific notation
options(scipen = 999)

```


# Exploring the Data

## Cleaning DC Land Cover Data

JAMIE 12/12: I took out our removal of all vars containing "_ac" because I needed these land variables for the metrics - will remove them later below.

```{r}

# Read in Urban Tree Canopy by Census Block Group (2020) data from Open Data DC
# and clean up
landcover <- read_sf(paste0("data/Urban_Tree_Canopy",
                            "_by_Census_Block_Group_in_2020.shp")) |>
  rename(block_group = GEOID) |>
  mutate(block_group = as.character(block_group)) |>
  mutate(OBJECTID = as.character(OBJECTID)) |>
  rename_all(tolower) |>
  relocate(block_group, .after = objectid) |>
  # Converting UHI to Fahrenheit
  mutate(uhi = (uhi*(9/5) + 32))

# Review vars for topline view and error coding
kable(landcover_summary <- as_tibble(landcover |>
                                       select(where(is.numeric)) |>
                                       st_drop_geometry() |>
                                       lapply(summary)) |>
        mutate(measure = c("Min", "Q1", "Median", "Mean", "Q3", "Max")) |>
        mutate(across(everything(), as.vector)) |>
        pivot_longer(-measure) |>
        pivot_wider(id_cols = "name",
                    names_from = "measure",
                    values_from = "value"))

```

```{r}

# Create key with Urban Tree Canopy data column descriptions and clean up
landcover_key <- read_csv("data/Urban_Tree_Canopy_Descriptions.csv",
                          col_names = FALSE) |>
  mutate(X1 = str_remove_all(X1, "\\( type:.*alias:|, length:.*| \\)")) |>
  separate(col = X1,
           into = c("field", "description"),
           sep = " ",
           extra = "merge") |>
  mutate(field = tolower(field))

```


Stephanie 12/10 - Downloaded temperature data from NASA's Earth Observations: https://neo.gsfc.nasa.gov/view.php?datasetId=MOD_LSTD_M&date=2020-07-01

## Exploring Census Demographic Data

```{r}

# Load ACS 5-yr 2016-2020 data
acs_variables <- load_variables(2020, "acs5") |>
  filter(geography == "block group")

```

MADELEINE 12/12: I added the following variables: Population by race (B02001...), total continuous HH income (B19001_001E), median property value (B25077_001E), total property value (	
B25075_001E), median gross rent as % HH income (B25071_001E), total educational attainment (B15003_001E). Some notes/thoughts:

* We obviously don't need both the income categories and the continuous income - I just wanted to add in everything for now for data exploration. We can always drop the variables we're not using later downstream
JAMIE 12/12 - I took out the category vars, let me know if anybody wants to add back in! I think the total measure would just give us the total line from the table that breaks down all income levels (for example, see the total line here: https://data.census.gov/table/ACSDT5Y2021.B19001?q=B19001). I added the median var below.

* Also prob don't need both total and median property values - just wanted to look at both
JAMIE 12/12 - Agreed, and here too, I don't think the total line would be super helpful (see total line here: https://data.census.gov/table?q=B25075) - I removed it to avoid confusion, hope that's ok!

* Also wondering if we should use median HH income instead of total?
JAMIE 12/12 - I added in median below!

* I believe the race categories I included should sum to the total population, but will test this
JAMIE 12/12 - I think based on my reading of the Census methodologies, they normally warn that they don't sum because of estimation and error at each level. But I think this would only a problem if we're trying to sum them and use the total. 

* I'm not sure what "total educational attainment" looks like - will do some testing there too. If it's hard to interpret or not in a useful format, we can use the categorical education variables (would probably want to consolidate them into fewer categories like HS or less, some college, BA+)
JAMIE 12/12 - Here too, normally this total line is just the sum of all individuals, just to provide a total line in the table for users who only access that table and need a sum (For example it would just represent the total line in this table: https://data.census.gov/table/ACSDT1Y2023.B15003?q=B15003). As I mentioned above re: race sums, they probably won't sum exactly.

* It occurs to me we may need to choose baseline categories for any categorical variables we include and leave those out? Not sure if that is only a requirement for line and linear regression?
JAMIE 12/12 - Good point! I'm adding stuff in below without factoring that in for now, but I think we def need to determine if we need to deal with this!

## Loading Census demographic data via API

JAMIE 12/12 - Unfortunately I think we have to code -66666s as NAs. See here: http://bit.ly/3DnlfIh
"-666666666: The estimate could not be computed because there were an insufficient number of sample observations...For a 5-year median estimate, the margin of error associated with a median was larger than the median itself." I'm going to take out the code turning them to 0s and put in my code converting them to NAs - let me know if you guys disagree. This starts to get into the MOE issues Professor Williams was talking about...could become a problem...

JAMIE 12/12 - We have some duplicative vars but leaving them in for now...

```{r}

# Secure Census API key (from .env file) for use in Census API call
load_dot_env()
credential <- Sys.getenv("census_api_key")

# Create url for API call
url <- str_glue(paste0("https://api.census.gov/data/2020/acs/acs5?get=",
             # Description of data point
             "NAME,",
             # TOTAL POPULATION ESTIMATE
             "B01003_001E,",
             # TOTAL POPULATION BY RACE - WHITE ALONE
             "B02001_002E,",
             # TOTAL POPULATION BY RACE - BLACK OR AFR AMER ALONE
             "B02001_003E,",
             # TOTAL POPULATION BY RACE - AMER IND AND AK NAT ALONE
             "B02001_004E,",
             # TOTAL POPULATION BY RACE - ASIAN ALONE
             "B02001_005E,",
             # TOTAL POPULATION BY RACE - NAT HAW OR PAC ISL ALONE
             "B02001_006E,",
             # TOTAL POPULATION BY RACE - OTHER ALONE + TWO OR MORE RACES
             "B02001_008E,",
             # TOTAL HISPANIC OR LATINO
             "B03003_001E,",
             # TOTAL POP WITH DOCTORATE DEGREE
             "B15003_025E,",
             # TOTAL POP WITH PROFESSIONAL SCHOOL DEGREE
             "B15003_024E,",
             # TOTAL POP WITH MASTER'S DEGREE
             "B15003_023E,",
             # TOTAL POP WITH BACHELORS DEGREE
             "B15003_022E,",
             # TOTAL POP WITH ASSOCIATES DEGREE
             "B15003_021E,",
             # TOTAL POP WITH REGULAR HIGH SCHOOL DIPLOMA
             "B15003_017E,",
             # TOTAL POP WITH GED OR ALTERNATIVE CREDENTIAL
             "B15003_018E,",
             # MEDIAN HH INCOME IN PAST 12 MOS
             "B19013_001E,",
             # TOTAL HOUSING UNITS
             "B25002_001E,",
             # TOTAL OCCUPIED HOUSING UNITS
             "B25002_002E,",
             # TOTAL VACANT HOUSING UNITS
             "B25002_003E,",
             # MEDIAN PROPERTY VALUE
             "B25077_001E,",
             # MEDIAN GROSS RENT
             "B25064_001E,",
             # MEDIAN GROSS RENT
             # AS A PERCENTAGE OF HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B25071_001E",
             # Limit to block-group-level data for DC
             "&for=block%20group:*&for=tract:*&in=state:11&in=county:001"))

# Use url to request data from API
acs_json <- GET(url = url)

# Check call status
http_status(acs_json)

# Save API JSON response as text
acs_json <- content(acs_json, as = "text")

# Save JSON as character matrix
acs_matrix <- fromJSON(acs_json)

# Convert matrix to tibble
acs_data <- as_tibble(acs_matrix[2:nrow(acs_matrix), ],
                      .name_repair = "minimal")

# Add variable names to tibble
names(acs_data) <- acs_matrix[1, ] |>
  str_replace(" ", "_")

```

```{r}

# Clean up ACS data
acs_data <- acs_data |>
  rename(total_pop = B01003_001E,
         race_white = B02001_002E,
         race_black = B02001_003E,
         race_ai_an = B02001_004E,
         race_asian = B02001_005E,
         race_nh_pi = B02001_006E,
         race_oth_mult = B02001_008E,
         race_eth_hisp = B03003_001E,
         ed_doct = B15003_025E,
         ed_prof_deg = B15003_024E,
         ed_master = B15003_023E,
         ed_bach = B15003_022E,
         ed_assoc = B15003_021E,
         ed_ged = B15003_018E,
         ed_reg_hsd = B15003_017E,
         med_hhi = B19013_001E,
         med_prop_val = B25077_001E,
         med_gross_rent = B25064_001E,
         med_rent_pct_inc = B25071_001E,
         total_hous_units = B25002_001E,
         vac_units = B25002_003E,
         occ_units = B25002_002E,
         block_group_temp = block_group) |>
  # Create block_group column to use for combining this with the other datasets
  mutate(block_group = paste(state,
                             county, 
                             tract, 
                             block_group_temp,
                             sep = "")) |>
  rename_all(tolower) |>
  relocate(block_group, .before = name) |>
  mutate_at(vars(total_pop:med_rent_pct_inc), as.numeric)

# Review vars for topline view and error coding
kable(acs_data_summary <- as_tibble(acs_data |>
  select(where(is.numeric)) |>
  lapply(summary)) |>
  mutate(measure = c("Min", "Q1", "Median", "Mean", "Q3", "Max")) |>
  mutate(across(everything(), as.vector)) |>
  pivot_longer(-measure) |>
  pivot_wider(id_cols = "name",
              names_from = "measure",
              values_from = "value"))

# NOTE ACS error coding: "The estimate could not be computed because there were
# an insufficient number of sample observations. For a ratio of medians
# estimate, one or both of the median estimates falls in the lowest interval or
# highest interval of an open-ended distribution. The estimate could not be
# computed because there were an insufficient number of sample observations.
# For a ratio of medians estimate, one or both of the median estimates falls in
# the lowest interval or highest interval of an open-ended distribution. For a
# 5-year median estimate, the margin of error associated with a median was
# larger than the median itself." (http://bit.ly/3DnlfIh)
acs_data <- acs_data |>
  mutate(med_hhi = if_else(med_hhi == -666666666, NA, med_hhi),
         med_prop_val = if_else(med_prop_val == -666666666, NA, med_prop_val),
         med_gross_rent = if_else(med_gross_rent == -666666666, NA, med_gross_rent),
         med_rent_pct_inc = if_else(med_rent_pct_inc == -666666666, NA, med_rent_pct_inc))

# Rerun summary measures
kable(acs_data_summary <- as_tibble(acs_data |>
                                      select(where(is.numeric)) |>
                                      lapply(summary) |>
                                      lapply(`length<-`, 6)) |>
        mutate(measure = c("Min", "Q1", "Median", "Mean", "Q3", "Max")) |>
        mutate(across(everything(), as.vector)) |>
        pivot_longer(-measure) |>
        pivot_wider(id_cols = "name",
                    names_from = "measure",
                    values_from = "value"))

```

## Stitching Together Final Dataset

JAMIE 12/12: Note I changed the name of the main df we are using pre-split to "dc_full_data." ALSO note that I moved the hotspot / not hotspot var to the below code chunk to streamline data cleanup.

Moved your threshold notes here:
STEPHANIE PRE-12/3
Since there's 571 block groups, that means if we want to decide the heat threshold is where 50% of the data is above/below this line, then we'd want to pick the UHI value where cumulative = 285 or 286. The closest datapoint is UHI = 7.9 (Where cumulative = 268). I vote that we pick UHI <= 7.9 means not a heat island whereas UHI > 7.9 means it is a heat island.

MADELEINE 12/4
We might want to consider a higher threshold? I'm thinking it would be more meaningful to point out, say, the hottest 25% of block groups vs. the top 50%.

MADELEINE 12/12
New 50% threshold (UHI data from landcover; C to F conversion) is 90.1.


```{r}

# Create final dataset and add metrics
dc_full_data <- left_join(landcover, acs_data, by = "block_group") |>
  mutate(
    # convert total acres to sq miles
    total_sq_mi = total_ac * 0.0015625,
    # convert water acres to sq miles
    water_sq_mi = wat_ac * 0.0015625,
    # calculate water share of total sq miles
    share_water_sq_mi = water_sq_mi / total_sq_mi,
    # calculate population density per square mile
    pop_dens_sq_mi = total_pop / total_sq_mi,
    # calculate population density per water sq mile
    pop_density_water_sq_mi = total_pop / water_sq_mi,
    # calculate ratio of housing units to population
    hous_units_per_person = total_hous_units / total_pop,
    # calculate ratio of vacant units to all units
    vac_units_share = vac_units / total_hous_units,
    # calculate ratio of total income to median home value
    inc_home_val_ratio = med_hhi / med_prop_val,
    # create combined HS diploma or equivalent variable
    ed_comb_hsd = ed_reg_hsd + ed_ged,
    # create combined advanced post-undergraduate variable
    ed_comb_adv_deg = ed_doct + ed_prof_deg + ed_master,
    # calculate ratio of 25+ pop with HSD to total pop
    ed_hsd_ratio = ed_comb_hsd / total_pop,
    # calculate ratio of 25+ pop with bachelor's degree to total pop
    ed_bach_ratio = ed_bach / total_pop,
    # calculate ratio of 25+ pop with advanced post-undergrad degree to total pop
    ed_adv_ratio = ed_comb_adv_deg / total_pop,
    # calculate pop share white
    race_white_ratio = race_white / total_pop,
    # calculate pop share black or african american
    race_black_ratio = race_black / total_pop,
    # calculate pop share american indian or alaska native
    race_ai_an_ratio = race_ai_an / total_pop,
    # calculate pop share asian
    race_asian_ratio = race_asian / total_pop,
    # calculate pop share native hawaiian or pacific islander
    race_nh_pi_ratio = race_nh_pi / total_pop,
    # calculate pop share two or more races
    race_oth_mult_ratio = race_oth_mult / total_pop,
    # calculate pop share hispanic or latino
    race_eth_hisp_ratio = race_eth_hisp / total_pop,
    # creating hotspot var for decision tree model
    hotspot = if_else(uhi <= 90.1, "hotspot", "not hotspot"),
    # remove factor from uhi var
    uhi = as.numeric(levels(uhi))[uhi]) |>
  relocate(c("block_group_temp", "tract"), .after = 2) |>
  # remove columns with only one unique value
  select(-objectid,
         -name,
         -state,
         -statefp,
         -county,
         -countyfp,
         -tractce,
         -blkgrpce,
         -namelsad,
         -mtfcc,
         -funcstat,
         -field,
         -shapearea,
         -shapelen) |>
  relocate(race_white_ratio:race_eth_hisp_ratio, .after = race_eth_hisp) |>
  relocate(ed_comb_hsd:ed_adv_ratio, .after = ed_ged) |>
  relocate(hous_units_per_person:inc_home_val_ratio, .after = med_rent_pct_inc) |>
  relocate(total_sq_mi:pop_density_water_sq_mi, .after = inc_home_val_ratio) |>
  relocate(total_pop:hotspot, .after = tract)

```

### Prepare the data for analysis

JAMIE 12/12 - Changed the var names so that these are the splits we use for all models, not just decision tree

* Do we need to separate into implementation df too?

```{r}

# Set seed
set.seed(20241202)

# Split sample and create training and testing datasets
dc_split <- initial_split(data = dc_full_data, prop = 0.8)
dc_train <- training(x = dc_split) 
dc_test <- testing(x = dc_split)

```

JAMIE 12/12 - Do we want folds? added here

```{r}

# V-fold cross validation with 10 folds
dc_folds <- vfold_cv(data = dc_train, v = 10)

```

JAMIE 12/12 - Since we can only do EDA on the training data - I moved the maps and vizs down here and changed the dfs to the training one!

## Initial EDA

### Exploration of some of the continuous variables

JAMIE 12/12 - Flag that we have a warning message that 56 rows were removed in below viz - just flagging for now.

```{r}

# Median household income, continuous
dc_train |>
  ggplot() +
  geom_histogram(aes(x = as.numeric(med_hhi))
  )

# Total educational attainment
#dc_train |>
#  ggplot() +
#  geom_histogram(aes(x = as.numeric(educ_total))
#  )
# Still not sure how to interpret this one. Might be worth going back to the categories

```

### DC Temperature Map

Stephanie 12/11 - We'll have to change the bottom code due to using different temperature data
MA 12/12 - Created new map using landcover 'uhi' variable for temperature, but only using 'landcover' as a dataset - need to figure out how to make 'dc_full_data' work

JAME 12/12 - I fixed this - I guess we have to left_join with the sf df in the first position in left_join(), so that we join the non-sf df onto the sf df and it stays sf. BUT do we have to st_transform() since the crs is not 4326?

```{r}

dc_train |> ggplot() +
  geom_sf(
    mapping = aes(fill = uhi),
    color = "white"
    ) +
    scale_fill_gradient(
    low = "#f7ef9d",
    high = "#be2b25"
    ) +
  labs(
    title = "Average evening temperature, 2018",
    fill = "Temperature (degrees F)"
  ) +
  theme_void()

```

### Decision Tree EDA

Stephanie 12/11 - We'll have to change the bottom code due to using different temperature data
Madeleine 12/12 - Updated this to work with UHI variable from landcover
Deciding the threshold for hot vs. not hot:

JAMIE 12/12 - Now getting an error below in cumulative... Do the cumulative numbers need to change since this is training data only?

```{r}

# visualizing a cumulative histogram of counts of UHI values
dc_train |>
  group_by(uhi) |>
  ggplot(aes(uhi)) +
  geom_histogram(aes(y = cumsum(..count..)))

# calculating frequency & cumulative frequency
heat_freq <- dc_train |>
  count(uhi) |>
  mutate(cumulative = cumsum(heat_freq$n))

heat_freq |>
  filter(
    cumulative >= 285,
    cumulative <= 286) |>
  head()

```

JAMIE 12/12 - I added back uhi here bc we need for the lm and rf models. I added a step_rm with uhi to the decision tree model instead.

```{r}

# Prep the dataset for modeling
dc_train <- dc_train |>
  st_drop_geometry() |>
  # dropping uhi because that would be the main decision tree factor
  select(-block_group, -intptlat, -intptlon) 

```


JAMIE 12/12 - Making a note that we need to do step_rm() here to remove:
- The vars ending in "_ac" that we wanted to remove originally
- vars that contain info that is duplicative of other vars (i know if we have month and full date, we'd have to remove one; do you think if we have a ratio metric, we have to remove the vars used in the ratio? I'm not clear on that.)
* Also changing the dataset used in the recipe to dt_train rather than the full df

## Decision Tree
```{r}

dt_recipe <- recipe(formula = hotspot ~ ., data = dc_train) |>
  step_rm(uhi) |>
  themis::step_downsample(hotspot) |> #I don't know if we need this or not...
  step_nzv(all_numeric_predictors())

dt_model <- decision_tree() |>
  set_engine(engine = "rpart") |>
  set_mode(mode = "classification")

dt_workflow <- workflow() |>
  add_recipe(dt_recipe) |>
  add_model(dt_model)
  
dt_fitting <- dt_workflow |>
  fit(data = dc_train)

rpart.plot::rpart.plot(x = dt_fitting$fit$fit$fit,
                       roundint = FALSE)

```

## Linear Regression

# Having trouble with a factor error below...
https://stackoverflow.com/questions/44200195/how-to-debug-contrasts-can-be-applied-only-to-factors-with-2-or-more-levels-er

```{r}

# Create the recipe
dc_recipe <- recipe(formula = uhi ~ ., data = dc_train)

# Model 2 - linear regression
dc_lm_model <- 
  linear_reg() |>
  set_mode(mode = "regression") |>
  set_engine(engine = "lm")

# Linear regression workflow
dc_lm_workflow <- 
  workflow() |>
  add_recipe(recipe = dc_recipe) |>
  add_model(spec = dc_lm_model)
  
unloadNamespace("Metrics")

# Fit the regression workflow
dc_lm_fit <-
  dc_lm_workflow |>
  fit_resamples(
    resamples = dc_folds,
    metrics = metric_set(rmse),
    control = control_resamples(save_workflow = TRUE))

```

## Random Forests

JAMIE 12/12 - I do not really know what numbers to put for things like trees and grid etc at the moment...just putting in what I used for the homework from class examples, for us to update later...

* And another error here...I'm worried it's because of NAs.

```{r}

# Model 3 - random forest
dc_rf_model <- 
  rand_forest(
  trees = 200,
  mtry = tune(),
  min_n = tune()) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

# Create a parameter grid
kable(dc_rf_grid <- grid_regular(mtry(range = c(1, 15)),
                                min_n(range = c(1, 15)),
                                levels = 5))

# Random forest workflow
dc_rf_workflow <- 
  workflow() |>
  add_recipe(dc_recipe) |>
  add_model(dc_rf_model)

# Fit the random forest workflow
dc_rf_fit <-
  dc_rf_workflow |>
  tune_grid(
    resamples = dc_folds,
    grid = dc_rf_grid,
    metrics = metric_set(rmse),
    control = control_grid(save_workflow = TRUE))

```

#### Plot the RMSEs across each fit:

JAMIE 12/12 - Below is what I used in the assignment 07 hw to pull the metrics in the end, so pasted here and updated a bit. Can't tell if it works until the models themselves can run...

```{r}

# Create a df with rmses for all models

# Extract rmses for rf approach
rf <- as_tibble(
  sapply(dc_rf_fit$.metrics, function(x) extract2(x, 1))) |>
  select(V1) |>
  rename(mtry = V1) |>
  bind_cols(as_tibble(
  sapply(dc_rf_fit$.metrics, function(x) extract2(x, 2)))) |>
  select(mtry, V1) |>
  rename(min_n = V1) |>
  bind_cols(as_tibble(
  sapply(dc_rf_fit$.metrics, function(x) extract2(x, 5)))) |>
  rename_all( ~ str_replace(., "V", "Fold0")) |>
  rename(Fold10 = Fold010) |>
  pivot_longer(cols = 3:12,
               names_to = "id",
               values_to = "rmse") |>
  mutate(model = "rf") |>
  select(id, mtry, min_n, model, rmse)

# Extract rmses for lm approaches
lm <- tibble(
  id = dc_lm_fit$id,
  lm = as_vector(sapply(dc_lm_fit$.metrics, function(x) extract2(x, 3)))) |>
  pivot_longer(cols = c("lm"),
               names_to = "model",
               values_to = "rmse") |>
  mutate(nn = NA,
         mtry = NA,
         min_n = NA) |>
  select(id, mtry, min_n, model, rmse)

# Create one df containing all rmses
fit_comps <- bind_rows(lm, rf)

# Plot the rmses
fit_comps |>
  ggplot(mapping = aes(x = model, y = rmse)) +
  geom_point() +
  labs(title = "RMSE of Linear Regression and Random Forest Models\n",
       subtitle = paste0("Prediction Model Evaluation for Washington, DC\n",
                         "Urban Heat Index (UHI)"),
       x = "Prediction Model",
       y = "RMSE")

# print average rmse per model
kable(avg_rmse <- tibble(
  model = c("lm", "rf"),
  rmse = c(mean(fit_comps$rmse[fit_comps$model=="lm"]),
           mean(fit_comps$rmse[fit_comps$model=="rf"]))) |>
    arrange(rmse))

```

### 2.4 Estimate the out-of-sample error rate

```{r}

library(Metrics)

# save best lm fit
best_lm_fit <- fit_best(dc_lm_fit, verbose = TRUE)

# make predictions using best fit
test_predictions <-
  dc_test |>
  select(uhi) |>
  rename(y = uhi) |>
  mutate(y = as.numeric(y)) |>
  mutate(y_hat = as.numeric(as_vector(predict(best_lm_fit, dc_test))))

# collect rf metrics
kable(rmse(actual = test_predictions$y, predicted = test_predictions$y_hat))

```


## Sources

https://www.climatecentral.org/climate-matters/urban-heat-islands-2024

https://opendata.dc.gov/datasets/DCGIS::urban-tree-canopy-by-census-block-group-in-2020/about

Census ACS 5yr 2016 - 2020

https://neo.gsfc.nasa.gov/view.php?datasetId=MOD_LSTD_M&date=2020-07-01

https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/MultiyearACSAccuracyofData2020.pdf


