---
title: "Final Project"
subtitle: "Data Science for Public Policy"
author: "Madeleine Adelson, Stephanie Jamilla, Jamie Jelly Murtha"
format: 
  html:
    code-line-numbers: true
    body-width: 1600px
embed-resources: true
editor_options: 
  chunk_output_type: console
execute: 
  warning: false
urlcolor: blue
toc: true
---

## About This Project
Climate Central's UHI index values (in °F) are "estimates of how much the urban built environment boosts temperatures. In other words, the UHI index is an estimate of the additional heat that local land use factors contribute to urban areas."

## Setup

Loading packages:
```{r}

library(dotenv)
library(tidyverse)
library(ggplot2)
library(sf)
library(tidycensus)
library(stringr)
library(jsonlite)
library(httr)
library(tidymodels)

# Clone the private github repository for this assignment using SSH link:
# git@github.com:stejamilla/final-project.git

# do not use scientific notation
options(scipen = 999)

```

# Exploring the Data

## Cleaning Urban Heat Data
```{r}

# Read in heat data and clean up columns
dc_heat <- read_sf("data/2024UHI_DC.kml") |>
  separate(Name, c("city","temp"), "- ") |>
  separate(temp, c("uhi", "temp"), "°") |>
  separate(temp, c("temp", "block_group", "x"), ": ") |>
  select(-city, -temp, -x, -Description) |>
  mutate(uhi = as.numeric(uhi)) |>
  relocate(block_group, .before = uhi) |>
  # set crs
  st_set_crs(value = 4326)

```

STEPHANIE PRE-12/3
I read more about (the methodology)[https://www.climatecentral.org/climate-matters/urban-heat-islands-2024] of this dataset and realized that Climate Central basically created the UHI index by analyzing things like land cover types, percentage of greenspace, albedo, population density, etc to create this dataset. So in a way, this project is almost seeing if we can replicate their model - which they used for 65 cities, specifically for DC. I think we can frame this as using DC-specific data and seeing if there are important factors Climate Central overlooked or if we think their model is valid. -Stephanie

JAMIE 12/3
I like this idea! That makes sense!

## Cleaning DC Land Cover Data
```{r}

# Read in Urban Tree Canopy by Census Block Group (2020) data from Open Data DC
# and clean up
landcover <- read_sf(paste0("data/Urban_Tree_Canopy",
                            "_by_Census_Block_Group_in_2020.shp")) |>
  rename(block_group = GEOID) |>
  # Drop other unrelated UHI index, which, here, measures average temperature
  # within each feature
  select(-UHI) |>
  mutate(block_group = as.character(block_group)) |>
  rename_all(tolower) |>
  # remove columns that provide measures in acres, to focus instead on
  # percentage measures
  select(-contains("_ac")) |>
  relocate(block_group, .after = objectid)

# Create key with Urban Tree Canopy data column descriptions and clean up
landcover_key <- read_csv("data/Urban_Tree_Canopy_Descriptions.csv",
                          col_names = FALSE) |>
  mutate(X1 = str_remove_all(X1, "\\( type:.*alias:|, length:.*| \\)")) |>
  separate(col = X1,
           into = c("field", "description"),
           sep = " ",
           extra = "merge") |>
  mutate(field = tolower(field))
  
```
STEPHANIE PRE-12/3
Notably, this dataset also has a UHI variable; its definition is "The average temperature value within each feature. Evening temperature values from a 2018 Portland State University study were used." Since this is average temperature, I don't think this really counts as an urban heat index/what we're really trying to analyze here. So I dropped it (so that it wouldn't be confused when we merged with the dc_heat dataset). -Stephanie
JAMIE 12/3 - Makes sense!

STEPHANIE PRE-12/3
I thought percentage of land cover (canopy, impervious surfaces, etc.) is better than actual amount of land cover since some block groups are larger/smaller than others - so percentages are more readily comparable.
JAMIE 12/3 = Great idea! I moved this code to the earlier cleanup chunk just to tidy things up.

## Exploring Census Demographic Data

```{r}

# Load ACS 5-yr 2016-2020 data
variables <- load_variables(2020, "acs5") |>
  filter(geography == "block group")

```
STEPHANIE PRE-12/3
I (Stephanie) think we could use some or all of the following ACS data:
* B01003_001 - Total population
* B02001_002 through B02001_010 - Population for different racial groups
* B15003_002 through B15003_025 - Educational attainment of those 25 and over
* B19001_002 through B19001_017 - Household income in the past 12 months
* B25001_001 - Total # of housing units
* B25002_002 & B25002_003 - Total # of occupied or vacant units
* B25075_002 through B25075_027 - Total value of housing units

I'll only load a few below for analysis since we haven't agreed on the variables yet, but I want to be able to play around with some data. -Stephanie

JAMIE 12/3 - I like these variables! Here are some ideas to scale them and make them relatable across areas and years:
* B01003_001 - Total population
- Log population growth since the previous 5-yr ACS
- Population per square mile or other distance (density)
- Population per total # of housing units

* B02001_002 through B02001_010 - Population for different racial groups
- Same thoughts as above

* B15003_002 through B15003_025 - Educational attainment of those 25 and over
- Share of population with each level of educational attainment
- Population with each level of educational attainment per square mile, or other distance (density of educational attainment)
- Population with each level of educational attainment per total # of housing units

* B19001_002 through B19001_017 - Household income in the past 12 months
- We just need to put in terms of one year's dollars - such as 2020 dollars
- Maybe we can use a metric that standardizes this relative to the living wage in the area (using a source like the MIT living wage calculator)

* B25075_002 through B25075_027 - Total value of housing units
- Same thoughts as above, need to put in terms of one year's dollars

If these are of interest, I'm happy to pull the data and add the calculations.

Loading Census variables via API:
```{r}

# Secure Census API key (from .env file) for use in Census API call
load_dot_env()
credential <- Sys.getenv("census_api_key")

### I think since we're not using the tidycensus package to obtain the ACS data,
### we don't need this code
# census_api_key(credential, install = TRUE, overwrite = TRUE)

# Create url for API call
url <- str_glue(paste0("https://api.census.gov/data/2020/acs/acs5?get=",
             # Description of data point
             "NAME,",
             # TOTAL POPULATION ESTIMATE
             "B01003_001E,",
             # TOTAL LESS THAN $10,000
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_002E,",
             # TOTAL $10,000 - $14,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_003E,",
             # TOTAL $15,000 - $19,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS   
             "B19001_004E,",
             # TOTAL $20,000 - $24,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS             
             "B19001_005E,",
             # TOTAL $25,000 - $29,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_006E,",
             # TOTAL $30,000 - $34,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS      
             "B19001_007E,",
             # TOTAL $35,000 - $39,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_008E,",
             # TOTAL $40,000 - $44,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_009E,",
             # TOTAL $45,000 - $49,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_010E,",
             # TOTAL $50,000 - $59,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_011E,",
             # TOTAL $60,000 - $74,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS       
             "B19001_012E,",
             # TOTAL $75,000 - $99,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_013E,",
             # TOTAL $100,000 - $124,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_014E,",
             # TOTAL $125,000 - $149,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_015E,",
             # TOTAL $150,000 - $199,999
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_016E,",
             # TOTAL $200,000 or more
             # HOUSEHOLD INCOME IN THE PAST 12 MONTHS
             "B19001_017E",
             # Limit to block-group-level data for DC
             "&for=block%20group:*&for=tract:*&in=state:11&in=county:001"))

# Use url to request data from API
acs_json <- GET(url = url)

# Check call status
http_status(acs_json)

# Save API JSON response as text
acs_json <- content(acs_json, as = "text")

# Save JSON as character matrix
acs_matrix <- fromJSON(acs_json)

# Convert matrix to tibble
acs_data <- as_tibble(acs_matrix[2:nrow(acs_matrix), ],
                      .name_repair = "minimal")

# Add variable names to tibble
names(acs_data) <- acs_matrix[1, ] |>
  str_replace(" ", "_")

# Clean up data
acs_data <- acs_data |>
  rename(total_pop = B01003_001E,
         nc_less_10000 = B19001_002E,
         inc_10000_14999 = B19001_003E,
         inc_15000_19999 = B19001_004E,
         inc_20000_24999 = B19001_005E,
         inc_25000_29999 = B19001_006E,
         inc_30000_34999 = B19001_007E,
         inc_35000_39999 = B19001_008E,
         inc_40000_44999 = B19001_009E,
         inc_45000_49999 = B19001_010E,
         inc_50000_59999 = B19001_011E,
         inc_60000_74999 = B19001_012E,
         inc_75000_99999 = B19001_013E,
         inc_100000_124999 = B19001_014E,
         inc_125000_149999 = B19001_015E,
         inc_150000_199999 = B19001_016E,
         inc_200000_more = B19001_017E,
         block_group_temp = block_group) |>
  # Create block_group column to use for combining this with the other datasets
  mutate(block_group = paste(state,
                             county, 
                             tract, 
                             block_group_temp,
                             sep = "")) |>
  rename_all(tolower) |>
  relocate(block_group, .before = name)
                             
```

## Stitching Together Final Dataset
STEPHANIE PRE-12/3
Noting this isn't final, but I just wanted to put something together to analyze in the interim
```{r}

# Combine datasets
dataset_interim <- left_join(landcover, acs_data, by = "block_group") |>
  st_transform(crs = 4326)

### ERROR - duplicating rows! Trying to figure out a fix...
# Create final dataset
dataset_final <- st_join(x = dc_heat,
                         y = dataset_interim,
                         join = st_intersects)

```

## Initial EDA
### DC Heat Map
```{r}

# DC heat map
dc_heat |>
  ggplot() +
  geom_sf(mapping = aes(fill = uhi)) +
  scale_fill_gradient(
    'Urban Heat Island Index',
    low = '#FCEBA6',
    high = '#AF1F29'
  ) +
  labs(title = "DC Urban Heat Island Intensity") +
  theme_minimal() 

```
STEPHANIE PRE-12/3
Notably, every block group in DC boosts temperatures bt 5.7 degrees or more. There's no block group that doesn't have a boost in temperature.

# Supervised Machine Learning Applications

## Decision Trees

Deciding the threshold for hot vs. not hot:
```{r}

# visualizing a cumulative histogram of counts of UHI values
dc_heat |>
  group_by(uhi) |>
  ggplot(aes(uhi)) +
  geom_histogram(aes(y = cumsum(..count..)))

# calculating frequency & cumulative frequency
heat_freq <- dc_heat |>
  count(uhi)
heat_freq$cumulative <- cumsum(heat_freq$n) 

```
STEPHANIE PRE-12/3
Since there's 571 block groups, that means if we want to decide the heat threshold is where 50% of the data is above/below this line, then we'd want to pick the UHI value where cumulative = 285 or 286. The closest datapoint is UHI = 7.9 (Where cumulative = 268). I vote that we pick UHI <= 7.9 means not a heat island whereas UHI > 7.9 means it is a heat island.

Implementing threshold:
```{r}

dataset_final <- dataset_final |>
  mutate(hotspot = if_else(uhi <= 7.9, "Hotspot", "Not Hotspot"))

dataset_dt <- dataset_final |>
  st_drop_geometry() |>
  select(-uhi, -block_group.x, -block_group.y, -blockgroup) 
#dropping uhi because that would be the main decision tree factor

```

### Running decision tree
STEPHANIE PRE-12/3
Note: We should do some more data cleaning on dataset_final before running the decision tree but it's getting late and I am tired, so I'm leaving it as is for now -Stephanie
```{r}

set.seed(20241202)

dt_split <- initial_split(data = dataset_dt, prop = 0.8)
dt_train <- training(x = dt_split) 
dt_test <- testing(x = dt_split)

dt_recipe <- recipe(formula = hotspot ~ ., data = dataset_dt) |>
  themis::step_downsample(hotspot) |> #I don't know if we need this or not...
  step_nzv(all_numeric_predictors())

dt_model <- decision_tree() |>
  set_engine(engine = "rpart") |>
  set_mode(mode = "classification")

dt_workflow <- workflow() |>
  add_recipe(dt_recipe) |>
  add_model(dt_model)
  
dt_fitting <- dt_workflow |>
  fit(data = dt_train)

rpart.plot::rpart.plot(x = dt_fitting$fit$fit$fit,
                       roundint = FALSE)

```

## Linear Regression
```{r}



```

## Random Forests
```{r}



```


## Sources

https://www.climatecentral.org/climate-matters/urban-heat-islands-2024

https://opendata.dc.gov/datasets/DCGIS::urban-tree-canopy-by-census-block-group-in-2020/about

Census ACS

